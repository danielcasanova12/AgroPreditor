# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y98NYwBP73Tad_udr2v37-BlyzWmfh-b
"""

# from google.colab import drive
# drive.mount('/content/drive')

"""Criando o arquivo NDVI_Municipios_unico.csv com todas as cidades e no periodo de setembro a março"""

import pandas as pd
import re
import logging

# Configuração de Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("execution.log"),
        logging.StreamHandler()
    ]
)

# Caminho do CSV original com a coluna 'municipio' "suja"
# file_path = '/content/drive/MyDrive/dataset/NDVI_Municipios_unico.csv'
# out_path = '/content/drive/MyDrive/dataset/NDVI_Municipios_filtrado.csv'

file_path = './data/NDVI_Municipios_unico.csv'
out_path = './data/NDVI_Municipios_filtrado.csv'

# Carrega o CSV
df = pd.read_csv(file_path)

# 1) Extrair só a parte do nome do município da string grande
# Ex.: "api_Municipios — camada_unida_Pinhal_de_São_Bento_1_1"
# fica "Pinhal_de_São_Bento"
df['municipio_raw'] = df['municipio'].str.extract(
    r'unida_(.+?)_\d+_\d+',
    expand=False
)

# Se não encontrou pelo padrão, mantém o valor original
df['municipio_raw'] = df['municipio_raw'].fillna(df['municipio'])

# 2) Deixar o nome mais legível: trocar "_" por espaço
# Ex.: "Pinhal_de_São_Bento" -> "Pinhal de São Bento"
df['municipio_nome'] = (
    df['municipio_raw']
    .str.replace('_', ' ', regex=False)
    .str.strip()
)

# 3) Lista de municípios de interesse
municipios_interesse = [
    "Lindoeste",
    "Bandeirantes",
    "Imbaú",
    "Antonina",
    "Pinhal de São Bento",
    "Nova Esperança do Sudoeste",
    "Campina do Simão",
    "Diamante do Norte",
    "Cruzeiro do Sul",
    "Wenceslau Braz",
    "Francisco Alves",
    "Moreira Sales",
    "Mato Rico",
    "Diamante do Sul",
]

# Filtra apenas linhas cujo município está na lista
df_filtrado = df[df['municipio_nome'].isin(municipios_interesse)].copy()

# 4) Define a coluna final 'municipio' e seleciona só as colunas desejadas
# Se você preferir COM ESPAÇOS (recomendado para juntar com outros dados):
df_filtrado['municipio'] = df_filtrado['municipio_nome']

# Se preferir com underscore, use em vez da linha acima:
# df_filtrado['municipio'] = df_filtrado['municipio_raw']

# Mantém só as colunas finais
df_filtrado = df_filtrado[['data', 'valor', 'municipio']]

# Salva o CSV ajustado
df_filtrado.to_csv(out_path, index=False, float_format='%.4f')
print(f"CSV ajustado e filtrado salvo em: {out_path}")
print(df_filtrado.head())

"""Carregando o csv de produtividade"""

import pandas as pd

# file_path = '/content/drive/MyDrive/dataset/soja_por_ano_municipio_area.csv'
# output_path = '/content/drive/MyDrive/dataset/soja_por_ano_municipio_area2.csv'

file_path = './data/soja_por_ano_municipio_area.csv'
output_path = './data/soja_por_ano_municipio_area2.csv'


try:
    df = pd.read_csv(file_path)
    print("Data loaded successfully!")

    print("Colunas encontradas:", df.columns.tolist())

    # Conferir se as colunas existem
    if "PRODUCAO" not in df.columns:
        raise ValueError("A coluna 'PRODUCAO' não existe no arquivo.")
    if "AREA TOTAL" not in df.columns:
        raise ValueError("A coluna 'AREA TOTAL' não existe no arquivo.")

    # Converter PRODUCAO e AREA TOTAL para número (lidando com vírgula)
    if df["PRODUCAO"].dtype == 'object':
        df["PRODUCAO"] = df["PRODUCAO"].str.replace(',', '.')
    if df["AREA TOTAL"].dtype == 'object':
        df["AREA TOTAL"] = df["AREA TOTAL"].str.replace(',', '.')

    df["PRODUCAO"] = pd.to_numeric(df["PRODUCAO"], errors="coerce")
    df["AREA TOTAL"] = pd.to_numeric(df["AREA TOTAL"], errors="coerce")

    # Fator de conversão: toneladas -> sacas (1 saca = 60 kg = 0,06 t)
    fator_ton_para_sacas = 1000 / 60  # ≈ 16.6667

    # produção em sacas por hectare
    df["producao"] = (df["PRODUCAO"] * fator_ton_para_sacas) / df["AREA TOTAL"]
    df["producao"] = df["producao"].round(2)

    # Remover colunas originais de produção e área
    df = df.drop(columns=["PRODUCAO", "AREA TOTAL"])

    # Renomear 'producao' -> 'PRODUCAO'
    df = df.rename(columns={"producao": "PRODUCAO"})

    # Salvar novo arquivo no Drive
    df.to_csv(output_path, index=False)
    print(f"Arquivo salvo em: {output_path}")

    print(df.head())

except FileNotFoundError:
    print(f"Error: The file at '{file_path}' was not found. Please check the path.")
except Exception as e:
    print(f"An error occurred: {e}")

import pandas as pd


# file_path = '/content/drive/MyDrive/dataset/clima_PR_2000-2024_clean.csv'
file_path = './data/clima_PR_2000-2024_clean.csv'


try:
    df = pd.read_csv(file_path)
    print("Data loaded successfully!")
    print(df.head())
except FileNotFoundError:
    print(f"Error: The file at '{file_path}' was not found. Please check the path.")
except Exception as e:
    print(f"An error occurred: {e}")

"""Gerando novo csv de dados climaticos sem null e no intervalo de datas de setembro a março

"""

import pandas as pd
import os

# file_path = '/content/drive/MyDrive/dataset/clima_PR_2000-2024_clean.csv'
file_path = './data/clima_PR_2000-2024_clean.csv'
# filtered_path = '/content/drive/MyDrive/dataset/clima_PR_2000-2024_setemar.csv'
filtered_path = './data/clima_PR_2000-2024_setemar.csv'




df = pd.read_csv(file_path)
print("Arquivo original carregado com sucesso!")

# Remove colunas totalmente nulas
df = df.dropna(axis=1, how='all')

# Remove colunas de Latitude e Longitude
df = df.drop(columns=['Latitude', 'Longitude'], errors='ignore')

# Converte coluna Data
df['Data'] = pd.to_datetime(df['Data'], errors='coerce')

# Remove linhas com Data inválida
df = df.dropna(subset=['Data'])

# Filtra meses de setembro a março
meses_safra = [9, 10, 11, 12, 1, 2, 3]
df_filtrado = df[df['Data'].dt.month.isin(meses_safra)]

# Salva arquivo filtrado
df_filtrado.to_csv(filtered_path, index=False)
print(f"Arquivo filtrado salvo em: {filtered_path}")

df = df_filtrado
print(df.head())

"""Juntando os dados de 3 formas diaria, mensal e anual"""





import pandas as pd
import os

# base_dir = '/content/drive/MyDrive/dataset'
base_dir = './data'


# ---------- Caminhos de entrada ----------
clima_path = os.path.join(base_dir, 'clima_PR_2000-2024_setemar.csv')
ndvi_path  = os.path.join(base_dir, 'NDVI_Municipios_filtrado.csv')
prod_path  = os.path.join(base_dir, 'soja_por_ano_municipio_area.csv')

# ---------- Caminhos de saída ----------
daily_out   = os.path.join(base_dir, 'soja_clima_ndvi_diario_final.csv')
monthly_out = os.path.join(base_dir, 'soja_clima_ndvi_mensal_final.csv')
annual_out  = os.path.join(base_dir, 'soja_clima_ndvi_anual_final.csv')

# ---------- 1) Carregar bases ----------

# Clima diário
df_clima = pd.read_csv(clima_path)
df_clima = df_clima.dropna(axis=1, how='all')  # remove colunas 100% nulas
df_clima['Data'] = pd.to_datetime(df_clima['Data'], errors='coerce')
df_clima = df_clima.dropna(subset=['Data'])
df_clima = df_clima.rename(columns={'Data': 'data', 'Municipio': 'municipio'})

# NDVI mensal (mas com datas pontuais no mês)
df_ndvi = pd.read_csv(ndvi_path)
df_ndvi['data'] = pd.to_datetime(df_ndvi['data'], errors='coerce')
df_ndvi = df_ndvi.dropna(subset=['data'])

# Produção anual (por safra)
df_prod = pd.read_csv(prod_path)
df_prod = df_prod.rename(columns={'Município': 'municipio', 'AREA TOTAL': 'AREA_TOTAL'})
df_prod['municipio'] = df_prod['municipio'].str.strip()

# ---------- 2) Função data -> SAFRA ----------

def date_to_safra(d):
    """
    Ex.: data em 2000-10 → safra '00/01'
         data em 2001-01 → safra '00/01'

    Considera safra de setembro (09) a março (03) do ano seguinte.
    """
    y = d.year
    m = d.month
    if m >= 9:  # set, out, nov, dez
        ano_ini = y
    else:       # jan, fev, mar
        ano_ini = y - 1
    ano_fim = ano_ini + 1
    return f"{ano_ini % 100:02d}/{ano_fim % 100:02d}"

df_clima['SAFRA'] = df_clima['data'].apply(date_to_safra)
df_ndvi['SAFRA']  = df_ndvi['data'].apply(date_to_safra)

# ---------- 3) Municípios de interesse ----------

municipios_interesse = [
    "Lindoeste",
    "Bandeirantes",
    "Imbaú",
    "Antonina",
    "Pinhal de São Bento",
    "Nova Esperança do Sudoeste",
    "Campina do Simão",
    "Diamante do Norte",
    "Cruzeiro do Sul",
    "Wenceslau Braz",
    "Francisco Alves",
    "Moreira Sales",
    "Mato Rico",
    "Diamante do Sul",
]

df_clima = df_clima[df_clima['municipio'].isin(municipios_interesse)].copy()
df_ndvi  = df_ndvi[df_ndvi['municipio'].isin(municipios_interesse)].copy()
df_prod  = df_prod[df_prod['municipio'].isin(municipios_interesse)].copy()

# ---------- 4) NDVI mensal (média por mês/município/safra) ----------

df_ndvi['ano'] = df_ndvi['data'].dt.year
df_ndvi['mes'] = df_ndvi['data'].dt.month

df_ndvi_mensal = (
    df_ndvi
    .groupby(['municipio', 'SAFRA', 'ano', 'mes'], as_index=False)['valor']
    .mean()
    .rename(columns={'valor': 'NDVI'})
)

# ---------- 5) Preparar clima diário com ano/mês ----------

df_clima['ano'] = df_clima['data'].dt.year
df_clima['mes'] = df_clima['data'].dt.month

# ---------- 6) Juntar clima diário + NDVI mensal (repetido por dia) ----------

df_daily = df_clima.merge(
    df_ndvi_mensal,
    on=['municipio', 'SAFRA', 'ano', 'mes'],
    how='left'
)

# ---------- 7) Juntar produção anual (repetida por dia da safra) ----------

df_daily = df_daily.merge(
    df_prod[['SAFRA', 'municipio', 'REGIAO', 'AREA_TOTAL', 'PRODUCAO']],
    on=['SAFRA', 'municipio'],
    how='left'
)

# ---------- 8) Garantir tipos numéricos e remover NaN relevantes ----------

for col in ['AREA_TOTAL', 'PRODUCAO', 'NDVI']:
    if df_daily[col].dtype == 'object':
        df_daily[col] = df_daily[col].str.replace(',', '.')
    df_daily[col] = pd.to_numeric(df_daily[col], errors='coerce')

# Aqui vem a regra "sem dados nullos":
# - só mantemos linhas em que NDVI, AREA_TOTAL e PRODUCAO existem
df_daily_clean = df_daily.dropna(subset=['NDVI', 'AREA_TOTAL', 'PRODUCAO']).copy()

# Salvar diário
df_daily_clean.to_csv(daily_out, index=False)
print("CSV diário final salvo em:", daily_out)

# ---------- 9) Criar tabela MENSAL ----------

group_cols_m = ['municipio', 'SAFRA', 'ano', 'mes']

climate_cols = [
    'Altitude (m)',
    'Tmax (°C)', 'Tmin (°C)', 'Tmed (°C)',
    'UR (%)', 'U2 (m/s)', 'RS (MJ/m²d)', 'Chuva (mm)'
]

agg_dict_m = {c: 'mean' for c in climate_cols}
agg_dict_m['Chuva (mm)'] = 'sum'   # CHUVA = SOMA do mês (não média!)
agg_dict_m['NDVI'] = 'mean'        # NDVI mensal médio (mas já vem mensal)
agg_dict_m['AREA_TOTAL'] = 'first' # área é anual, repetir por mês
agg_dict_m['PRODUCAO']   = 'first' # produção é anual, repetir por mês

df_monthly = (
    df_daily_clean
    .groupby(group_cols_m, as_index=False)
    .agg(agg_dict_m)
)

# adicionar Solo e REGIAO (primeiro valor da combinação)
for col in ['Solo', 'REGIAO']:
    if col in df_daily_clean.columns:
        aux = (
            df_daily_clean
            .groupby(group_cols_m, as_index=False)[col]
            .first()
        )
        df_monthly = df_monthly.merge(aux, on=group_cols_m, how='left')

df_monthly.to_csv(monthly_out, index=False)
print("CSV mensal final salvo em:", monthly_out)

# ---------- 10) Criar tabela ANUAL (por SAFRA) ----------

group_cols_a = ['municipio', 'SAFRA']

agg_dict_a = {c: 'mean' for c in climate_cols}
agg_dict_a['Chuva (mm)'] = 'sum'   # CHUVA = SOMA do ciclo (Set-Mar), não média!
agg_dict_a['NDVI'] = 'mean'        # NDVI médio na safra
agg_dict_a['AREA_TOTAL'] = 'first' # área da safra
agg_dict_a['PRODUCAO']   = 'first' # produção da safra

df_annual = (
    df_daily_clean
    .groupby(group_cols_a, as_index=False)
    .agg(agg_dict_a)
)

for col in ['Solo', 'REGIAO']:
    if col in df_daily_clean.columns:
        aux = (
            df_daily_clean
            .groupby(group_cols_a, as_index=False)[col]
            .first()
        )
        df_annual = df_annual.merge(aux, on=group_cols_a, how='left')

df_annual.to_csv(annual_out, index=False)
print("CSV anual final salvo em:", annual_out)

import pandas as pd
import os

#  base_dir = '/content/drive/MyDrive/dataset'
base_dir = './data'

daily_out   = os.path.join(base_dir, 'soja_clima_ndvi_diario_final.csv')
monthly_out = os.path.join(base_dir, 'soja_clima_ndvi_mensal_final.csv')
annual_out  = os.path.join(base_dir, 'soja_clima_ndvi_anual_final.csv')

# Carrega
df_daily   = pd.read_csv(daily_out)
df_monthly = pd.read_csv(monthly_out)
df_annual  = pd.read_csv(annual_out)

print("Daily shape  :", df_daily.shape)
print("Monthly shape:", df_monthly.shape)
print("Annual shape :", df_annual.shape)

# Função pra relatar % de nulos
def missing_report(df, name):
    miss = df.isna().mean().sort_values(ascending=False) * 100
    print(f"\n==== MISSING {name} (%) ====")
    print(miss)

missing_report(df_daily, "DAILY_FINAL")
missing_report(df_monthly, "MONTHLY_FINAL")
missing_report(df_annual, "ANNUAL_FINAL")

import numpy as np
import matplotlib.pyplot as plt

def plot_corr_matrix(df, name):
    df_temp = df.copy()

    # Converter colunas numéricas se necessário
    cols_to_numeric = ['PRODUCAO', 'AREA_TOTAL']
    for c in cols_to_numeric:
        if c in df_temp.columns:
            df_temp[c] = pd.to_numeric(df_temp[c], errors='coerce')

    # Criar produtividade sc/ha se possível
    if 'PRODUCAO' in df_temp.columns and 'AREA_TOTAL' in df_temp.columns:
        # (Ton * 1000 / 60) / Ha
        df_temp['prod_sc_ha'] = (df_temp['PRODUCAO'] * 1000 / 60) / df_temp['AREA_TOTAL']
    
    # === ADICIONAR FEATURES DE SINERGIA (INLINE) ===
    # Calculando as principais sinergias diretamente aqui
    if 'Tmax (°C)' in df_temp.columns and 'Tmin (°C)' in df_temp.columns:
        df_temp['GDD'] = ((df_temp['Tmax (°C)'] + df_temp['Tmin (°C)']) / 2) - 10
        df_temp['GDD'] = df_temp['GDD'].clip(lower=0)
    
    if 'Tmax (°C)' in df_temp.columns and 'Tmin (°C)' in df_temp.columns and 'UR (%)' in df_temp.columns:
        es = 0.6108 * np.exp((17.27 * df_temp['Tmax (°C)']) / (df_temp['Tmax (°C)'] + 237.3))
        ea = es * (df_temp['UR (%)'] / 100)
        df_temp['VPD'] = es - ea
    
    # Chuva acumulada 90d
    if 'Chuva (mm)' in df_temp.columns:
        df_temp['Chuva_Acum_90d'] = df_temp['Chuva (mm)'].rolling(90, min_periods=1).sum()
    
    # GDD acumulado 90d
    if 'GDD' in df_temp.columns:
        df_temp['GDD_Acum_90d'] = df_temp['GDD'].rolling(90, min_periods=1).sum()
    
    # Interações
    if 'Chuva_Acum_90d' in df_temp.columns and 'GDD_Acum_90d' in df_temp.columns:
        df_temp['Interacao_Chuva_GDD'] = df_temp['Chuva_Acum_90d'] * df_temp['GDD_Acum_90d']
    
    if 'NDVI' in df_temp.columns and 'RS (MJ/m²d)' in df_temp.columns:
        df_temp['NDVI_x_RS'] = df_temp['NDVI'] * df_temp['RS (MJ/m²d)']
    
    if 'NDVI' in df_temp.columns and 'Chuva_Acum_90d' in df_temp.columns:
        df_temp['NDVI_x_Chuva'] = df_temp['NDVI'] * df_temp['Chuva_Acum_90d']
    
    if 'Tmed (°C)' in df_temp.columns:
        df_temp['Tmed_Quadrado'] = df_temp['Tmed (°C)'] ** 2
        df_temp['Desvio_Temp_Otima'] = (df_temp['Tmed (°C)'] - 24) ** 2
    
    # Heat Stress Days
    if 'Tmax (°C)' in df_temp.columns:
        df_temp['Heat_Stress_30d'] = (df_temp['Tmax (°C)'] > 34).rolling(30, min_periods=1).sum()
    
    # Dias Secos
    if 'Chuva (mm)' in df_temp.columns:
        df_temp['Dias_Secos_30d'] = (df_temp['Chuva (mm)'] < 1).rolling(30, min_periods=1).sum()
    
    # Remover colunas proibidas
    forbidden = ['PRODUCAO', 'AREA_TOTAL', 'PRODUCAO_TON', 'AREA', 'Safra_ano', 'ano', 'mes']
    df_temp = df_temp.drop(columns=forbidden, errors='ignore')

    if 'Solo' in df_temp.columns:
        df_temp['Solo'] = df_temp['Solo'].astype('category').cat.codes
    
    # Remover colunas de identificação textual
    df_temp = df_temp.select_dtypes(include='number')
    
    # === SELECIONAR TOP FEATURES + SINERGIAS ===
    # Para não ficar muito grande, selecionamos as mais importantes
    features_corr = ['prod_sc_ha', 'NDVI', 'Chuva (mm)', 'RS (MJ/m²d)', 'Tmed (°C)',
                     'GDD', 'VPD', 'Chuva_Acum_90d', 'GDD_Acum_90d',
                     'Interacao_Chuva_GDD', 'NDVI_x_RS', 'NDVI_x_Chuva',
                     'Tmed_Quadrado', 'Desvio_Temp_Otima',
                     'Heat_Stress_30d', 'Dias_Secos_30d',
                     'Chuva_Ano_Anterior', 'NDVI_Ano_Anterior']
    
    cols_existentes = [c for c in features_corr if c in df_temp.columns]
    df_corr = df_temp[cols_existentes]

    corr = df_corr.corr()

    plt.figure(figsize=(14, 12))
    im = plt.imshow(corr.values, interpolation='nearest', cmap='coolwarm', vmin=-1, vmax=1)
    plt.title(f"Matriz de Correlação - {name} (com Sinergias)")
    plt.colorbar(im)
    plt.xticks(np.arange(len(corr.columns)), corr.columns, rotation=90, fontsize=9)
    plt.yticks(np.arange(len(corr.columns)), corr.columns, fontsize=9)
    
    # Adicionar valores na matriz
    for i in range(len(corr.columns)):
        for j in range(len(corr.columns)):
            text = plt.text(j, i, f'{corr.values[i, j]:.2f}',
                           ha='center', va='center', color='black', fontsize=6)

    plt.tight_layout()
    plt.savefig(f'./corr_matrix_{name.replace(" ", "_")}.png', dpi=150)
    plt.show()
    return corr

corr_daily   = plot_corr_matrix(df_daily,   "Diário")
corr_monthly = plot_corr_matrix(df_monthly, "Mensal")
corr_annual  = plot_corr_matrix(df_annual,  "Anual")

import pandas as pd
import os

#base_dir = '/content/drive/MyDrive/dataset'
base_dir = './data'

paths = {
    "Diário": os.path.join(base_dir, 'soja_clima_ndvi_diario_final.csv'),
    "Mensal": os.path.join(base_dir, 'soja_clima_ndvi_mensal_final.csv'),
    "Anual":  os.path.join(base_dir, 'soja_clima_ndvi_anual_final.csv'),
}

for nome, path in paths.items():
    print(f"\n{'='*20} {nome.upper()} {'='*20}")

    try:
        df = pd.read_csv(path)

        print(f"Caminho: {path}")
        print(f"Dimensões (Linhas, Colunas): {df.shape}")

        print(df.head())
    except Exception as e:
        print(f"Erro ao ler o arquivo {nome}: {e}")



import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV
import os

# XAI
import shap
import matplotlib.pyplot as plt

# ================= CONFIGURAÇÃO =================
#base_dir = '/content/drive/MyDrive/dataset'
base_dir = './data'
arquivos = {
    "DIÁRIO": os.path.join(base_dir, 'soja_clima_ndvi_diario_final.csv'),
    "MENSAL": os.path.join(base_dir, 'soja_clima_ndvi_mensal_final.csv'),
    "ANUAL":  os.path.join(base_dir, 'soja_clima_ndvi_anual_final.csv')
}

def add_agronomic_features(df):
    """
    Adiciona features agronômicas: GDD, VPD, Chuva Acumulada.
    """
    df = df.copy()
    
    # 1. GDD (Growing Degree Days) - Base 10°C (Soja)
    # GDD = ((Tmax + Tmin) / 2) - 10
    if 'Tmax (°C)' in df.columns and 'Tmin (°C)' in df.columns:
        tmean = (df['Tmax (°C)'] + df['Tmin (°C)']) / 2
        df['GDD'] = tmean - 10
        df['GDD'] = df['GDD'].apply(lambda x: max(x, 0)) # GDD não pode ser negativo
    
    # 2. VPD (Vapor Pressure Deficit) - kPa
    # es = 0.6108 * exp(17.27 * T / (T + 237.3))
    # VPD = es * (1 - UR/100)
    if 'Tmed (°C)' in df.columns and 'UR (%)' in df.columns:
        tmed = df['Tmed (°C)']
        ur = df['UR (%)']
        es = 0.6108 * np.exp((17.27 * tmed) / (tmed + 237.3))
        df['VPD'] = es * (1 - ur / 100)
        df['VPD'] = df['VPD'].apply(lambda x: max(x, 0))

    # 3. Chuva Acumulada (Rolling Windows)
    # Só faz sentido se tivermos ordem temporal. 
    # Assumindo que o DF já vem ordenado ou agrupado por município/safra se for processamento em lote.
    # Para simplificar no 'rodar_xgb' (que pega o dataset todo), vamos aplicar por município se possível.
    # Mas cuidado: rodar_xgb recebe o dataset já carregado.
    
    if 'Chuva (mm)' in df.columns:
        # Se tiver índice temporal ou estiver ordenado, o rolling funciona.
        # Vamos criar janelas móveis simples (30, 60, 90 dias)
        # Idealmente deveria ser groupby(municipio).rolling, mas aqui vamos tentar direto se o df estiver ordenado.
        # Para evitar misturar municípios, o ideal é fazer isso NA PREPARAÇÃO (rodar_hibrido faz isso).
        pass 

    # --- PHENOLOGICAL STRESS FEATURES ---
    
    # 1. Heat Stress: Dias acima de 34°C (Shutdown da fotossíntese)
    if 'Tmax (°C)' in df.columns:
        df['Heat_Stress'] = (df['Tmax (°C)'] > 34).astype(int)
        # Acumulado móvel de 30 dias de estresse térmico
        if 'municipio' in df.columns:
             df['Heat_Stress_30d'] = df.groupby('municipio')['Heat_Stress'].transform(lambda x: x.rolling(30, min_periods=1).sum())

    # 2. Critical Window Rain (Fase Reprodutiva: ~30-90 dias)
    # Aproximação: Acumulado de 60 dias (Janela crítica)
    # Já temos Chuva_Acum_60d, vamos criar uma feature explícita de "Deficit Reprodutivo"
    # Se chover menos de 150mm em 60 dias, é crítico.
    if 'Chuva_Acum_60d' in df.columns:
        df['Seca_Reprodutiva'] = (df['Chuva_Acum_60d'] < 150).astype(int)

    # 3. Drought Cycles (Dias consecutivos sem chuva)
    # Complexo fazer vetorizado exato sem iterar, vamos aproximar com:
    # "Quantos dias choveu < 1mm nos últimos 30 dias"
    if 'Chuva (mm)' in df.columns:
        df['Dias_Secos'] = (df['Chuva (mm)'] < 1).astype(int)
        if 'municipio' in df.columns:
             df['Dias_Secos_30d'] = df.groupby('municipio')['Dias_Secos'].transform(lambda x: x.rolling(30, min_periods=1).sum())

    # --- NDVI × CLIMA SYNERGIES ---
    # Hipótese: NDVI alto só se traduz em produção se houver recursos (água, calor acumulado).
    # NDVI verde mas seco = planta estressada, não produz.
    
    if 'NDVI' in df.columns:
        # NDVI × Chuva Acumulada (Vegetação verde + água = produção)
        if 'Chuva_Acum_90d' in df.columns:
            df['NDVI_x_Chuva'] = df['NDVI'] * df['Chuva_Acum_90d']
        
        # NDVI × GDD Acumulado (Vegetação verde + calor acumulado = maturação)
        if 'GDD_Acum_90d' in df.columns:
            df['NDVI_x_GDD'] = df['NDVI'] * df['GDD_Acum_90d']
        
        # NDVI × Radiação Solar (Fotossíntese efetiva)
        if 'RS (MJ/m²d)' in df.columns:
            df['NDVI_x_RS'] = df['NDVI'] * df['RS (MJ/m²d)']

    # --- POLYNOMIAL FEATURES ---
    # Hipótese: Existe um "ponto ótimo" para temperatura e NDVI.
    # Temperaturas muito baixas OU muito altas prejudicam. Termos quadráticos capturam essa parábola.
    
    if 'Tmed (°C)' in df.columns:
        df['Tmed_Quadrado'] = df['Tmed (°C)'] ** 2
    
    if 'NDVI' in df.columns:
        df['NDVI_Quadrado'] = df['NDVI'] ** 2
    
    # Desvio da temperatura ótima (~24°C para soja)
    if 'Tmed (°C)' in df.columns:
        df['Desvio_Temp_Otima'] = (df['Tmed (°C)'] - 24) ** 2

    # --- LAG FEATURES (Reserva Hídrica / Condições do Ano Anterior) ---
    # Hipótese: Chuva do ano anterior influencia reserva hídrica do solo.
    # Implementação: Fazer shift por SAFRA (dados anuais) ou aproximar nos diários.
    
    # Para dados com coluna SAFRA (anual/mensal), calculamos lag por município
    if 'SAFRA' in df.columns and 'Chuva (mm)' in df.columns:
        # Média de chuva por safra/município, depois shift
        df_safra_avg = df.groupby(['municipio', 'SAFRA'])['Chuva (mm)'].mean().reset_index(name='Chuva_Media_Safra')
        df_safra_avg['Chuva_Ano_Anterior'] = df_safra_avg.groupby('municipio')['Chuva_Media_Safra'].shift(1)
        df = df.merge(df_safra_avg[['municipio', 'SAFRA', 'Chuva_Ano_Anterior']], on=['municipio', 'SAFRA'], how='left')
    
    # NDVI do ano anterior (saúde do solo)
    if 'SAFRA' in df.columns and 'NDVI' in df.columns:
        df_ndvi_avg = df.groupby(['municipio', 'SAFRA'])['NDVI'].mean().reset_index(name='NDVI_Media_Safra')
        df_ndvi_avg['NDVI_Ano_Anterior'] = df_ndvi_avg.groupby('municipio')['NDVI_Media_Safra'].shift(1)
        df = df.merge(df_ndvi_avg[['municipio', 'SAFRA', 'NDVI_Ano_Anterior']], on=['municipio', 'SAFRA'], how='left')

    # === RESERVA HÍDRICA DO SOLO (Usando dados anuais completos) ===
    # Baseado em Embrapa (2010, 2022, 2024):
    # - Profundidade efetiva de raizes: 0.5m (Latossolo)
    # - Água Disponível (AD): ~1.3 mm/cm para solos argilosos (média Latossolo/Nitossolo)
    # - Capacidade de Armazenamento: AD * Prof = 1.3 * 50 = 65mm
    # - ETc soja: ~3.18 mm/dia (Amaral & Pilau, 2024)
    # - Kc médio: ~0.80 (Embrapa, 2020)
    
    try:
        # Carregar dados climáticos completos (12 meses)
        df_clima_completo = pd.read_csv(
            './data/dados_climaticos_parana_completo.csv',
            sep=';', decimal=','
        )
        df_clima_completo['Data'] = pd.to_datetime(df_clima_completo['Data'])
        df_clima_completo['ano'] = df_clima_completo['Data'].dt.year
        
        # Normalizar nomes de municípios
        df_clima_completo['municipio'] = df_clima_completo['Municipio'].str.strip().str.lower()
        
        # Parâmetros Embrapa para cálculo do Balanço Hídrico Simplificado
        PROF_RAIZ = 50  # cm (profundidade efetiva das raízes)
        AD_MEDIA = 1.3  # mm/cm (água disponível média para solos argilosos do PR)
        CAD = PROF_RAIZ * AD_MEDIA  # Capacidade de Água Disponível = 65mm
        KC_MEDIO = 0.80  # Coeficiente de cultivo médio da soja
        
        # Calcular Reserva Hídrica Anual por município
        # Reserva = Chuva_Anual - ETo_Anual * Kc (simplificado)
        # Se ETo não estiver disponível, usamos apenas a chuva acumulada
        
        if 'ETo (mm/d)' in df_clima_completo.columns:
            df_clima_completo['ETo (mm/d)'] = pd.to_numeric(df_clima_completo['ETo (mm/d)'], errors='coerce').fillna(0)
            # ETc = ETo * Kc
            df_clima_completo['ETc'] = df_clima_completo['ETo (mm/d)'] * KC_MEDIO
        else:
            df_clima_completo['ETc'] = 0
        
        # Agrupar por ano e município para calcular balanço anual
        df_reserva = df_clima_completo.groupby(['municipio', 'ano']).agg({
            'Chuva (mm)': 'sum',  # Total de chuva no ano
            'ETc': 'sum'  # Total de evapotraspiração
        }).reset_index()
        
        # Balanço Hídrico Simplificado: Excedente = Chuva - ETc
        # Limitado pela capacidade do solo (CAD)
        df_reserva['Excedente_Hidrico'] = df_reserva['Chuva (mm)'] - df_reserva['ETc']
        df_reserva['Reserva_Hidrica'] = df_reserva['Excedente_Hidrico'].clip(lower=0, upper=CAD * 12)  # Max = CAD * meses
        
        # Shift para pegar reserva do ano anterior
        df_reserva['Reserva_Hidrica_Ano_Anterior'] = df_reserva.groupby('municipio')['Reserva_Hidrica'].shift(1)
        df_reserva['Chuva_Anual_Anterior'] = df_reserva.groupby('municipio')['Chuva (mm)'].shift(1)
        
        # Converter ano para SAFRA (ano/ano+1 -> "XX/YY")
        def ano_para_safra(ano):
            return f"{str(ano)[-2:]}/{str(ano+1)[-2:]}"
        
        df_reserva['SAFRA'] = df_reserva['ano'].apply(ano_para_safra)
        
        # Merge com o dataframe principal
        if 'SAFRA' in df.columns:
            df['municipio_lower'] = df['municipio'].str.strip().str.lower() if 'municipio' in df.columns else ''
            df = df.merge(
                df_reserva[['municipio', 'SAFRA', 'Reserva_Hidrica_Ano_Anterior', 'Chuva_Anual_Anterior']],
                left_on=['municipio_lower', 'SAFRA'],
                right_on=['municipio', 'SAFRA'],
                how='left',
                suffixes=('', '_reserva')
            )
            # Limpar colunas temporárias
            df = df.drop(columns=['municipio_lower', 'municipio_reserva'], errors='ignore')
        
        logging.info(f"Reserva Hídrica calculada. Média: {df['Reserva_Hidrica_Ano_Anterior'].mean() if 'Reserva_Hidrica_Ano_Anterior' in df.columns else 'N/A'}")
        
    except Exception as e:
        logging.warning(f"Não foi possível calcular Reserva Hídrica: {e}")

    return df

def rodar_xgb(nome_dataset, caminho_csv):
    logging.info(f"{'#'*20} PROCESSANDO (XGBoost puro): {nome_dataset} {'#'*20}")

    # 1. Carga
    try:
        df = pd.read_csv(caminho_csv)
    except FileNotFoundError:
        print(f"Arquivo não encontrado: {caminho_csv}")
        return None

    # 2. Ordenação (só pra manter consistência)
    cols_ordenacao = ['municipio', 'SAFRA']
    if 'data' in df.columns:
        df['data'] = pd.to_datetime(df['data'], errors='coerce')
        cols_ordenacao.append('data')
    elif 'anomes' in df.columns:
        cols_ordenacao.append('anomes')

    df = df.sort_values(by=cols_ordenacao)

    # 3. Alvo: produtividade (kg/ha) + produção real
    df['PRODUCAO'] = pd.to_numeric(df['PRODUCAO'], errors='coerce')
    df['AREA_TOTAL'] = pd.to_numeric(df['AREA_TOTAL'], errors='coerce')
    df = df.dropna(subset=['PRODUCAO', 'AREA_TOTAL'])
    df = df[df['AREA_TOTAL'] > 0]

    # YIELD_REAL agora em sacas/ha (1 saca = 60kg = 0.06t)
    # Se PRODUCAO está em toneladas: (PRODUCAO * 1000 / 60) / AREA
    df['YIELD_REAL'] = (df['PRODUCAO'] * 1000 / 60) / df['AREA_TOTAL']
    logging.info(f"Target YIELD_REAL (sc/ha) calculado. Média: {df['YIELD_REAL'].mean():.2f}")

    # --- OUTLIER REMOVAL ---
    # Remover safras com produtividade muito baixa (< 10 sc/ha) que provavelmente são erros ou quebras totais
    # Isso ajuda o modelo a focar no clima "normal" vs "bom", sem ser confundido por desastres extremos.
    if 'YIELD_REAL' in df.columns:
        original_len = len(df)
        df = df[df['YIELD_REAL'] >= 10]
        logging.info(f"Removidos {original_len - len(df)} outliers (YIELD < 10 sc/ha).")

    # Adicionar features agronômicas (GDD, VPD, etc.)
    df = add_agronomic_features(df)

    # 3. Feature Engineering (Janelas Móveis)
    # Garante que temos dados suficientes para rolling
    if 'GDD' in df.columns:
        df['GDD'] = df['GDD'].fillna(0)
    if 'Chuva (mm)' in df.columns:
        df['Chuva (mm)'] = df['Chuva (mm)'].fillna(0)
    # Chuva acumulada (Rolling) - precisa ser feito com cuidado por município
    # Como rodar_xgb é "XGBoost Puro" (tabular), features temporais como rolling sums são vitais.
    if 'municipio' in df.columns and 'Chuva (mm)' in df.columns:
        # Ordenar para garantir rolling correto
        cols_sort = ['municipio', 'SAFRA']
        if 'data' in df.columns: cols_sort.append('data')
        elif 'anomes' in df.columns: cols_sort.append('anomes')
        
        df = df.sort_values(by=cols_sort)
        
        # Calcular rolling por município
        for window in [30, 60, 90]:
            df[f'Chuva_Acum_{window}d'] = df.groupby('municipio')['Chuva (mm)'].transform(lambda x: x.rolling(window, min_periods=1).sum())
            df[f'GDD_Acum_{window}d'] = df.groupby('municipio')['GDD'].transform(lambda x: x.rolling(window, min_periods=1).sum())

    features_clima = ['Tmax (°C)', 'Tmin (°C)', 'Tmed (°C)', 'UR (%)',
                      'U2 (m/s)', 'RS (MJ/m²d)', 'Chuva (mm)', 'NDVI',
                      'GDD', 'VPD', 
                      'Chuva_Acum_30d', 'Chuva_Acum_60d', 'Chuva_Acum_90d',
                      'GDD_Acum_30d', 'GDD_Acum_60d', 'GDD_Acum_90d']

    cols_clima = [c for c in features_clima if c in df.columns]
    if not cols_clima:
        logging.error("Nenhuma feature climática encontrada.")
        return None

    # Normaliza só clima/NDVI
    scaler = MinMaxScaler()
    df[cols_clima] = scaler.fit_transform(df[cols_clima].fillna(0))

    # 5. Outras features numéricas que podem ajudar
    extra_num = []
    # REMOVIDO AREA_TOTAL para evitar vazamento/correlação direta
    # if 'AREA_TOTAL' in df.columns:
    #     extra_num.append('AREA_TOTAL')

    # 6. Features categóricas
    cat_cols = []
    for c in ['municipio', 'SAFRA', 'Solo', 'REGIAO']:
        if c in df.columns:
            cat_cols.append(c)
            df[c] = df[c].astype('category')

    # Codifica categorias como inteiros
    for c in cat_cols:
        df[c + '_cat'] = df[c].cat.codes

    # Monta lista final de features
    feat_cols = cols_clima + extra_num + [c + '_cat' for c in cat_cols]

    logging.info(f"Features usadas ({len(feat_cols)}): {feat_cols}")

    # 7. Flag de teste (safra 23/24 ou 2024)
    def is_safra_2024(s):
        s_str = str(s)
        if s_str == '23/24' or s_str == '2024':
            return 1
        try:
            if int(s) == 2024:
                return 1
        except Exception:
            pass
        return 0

    df['is_2024'] = df['SAFRA'].apply(is_safra_2024)

    # 8. Split treino / teste
    df_train = df[df['is_2024'] == 0].copy()
    df_test  = df[df['is_2024'] == 1].copy()

    if df_train.empty or df_test.empty:
        logging.warning("Treino ou teste vazio (sem safra 2024 ou sem histórico).")
        return None

    X_train = df_train[feat_cols]
    y_train = df_train['YIELD_REAL']

    X_test  = df_test[feat_cols]
    y_test_prod_real = df_test['PRODUCAO']
    test_areas = df_test['AREA_TOTAL']

    # 9. Treinamento XGBoost com Tuning
    logging.info("Iniciando Tuning do XGBoost...")
    param_dist = {
        'n_estimators': [100, 300, 500],
        'learning_rate': [0.01, 0.05, 0.1],
        'max_depth': [3, 5, 7],
        'subsample': [0.7, 0.8, 0.9],
        'colsample_bytree': [0.7, 0.8, 0.9]
    }
    
    xgb_model = xgb.XGBRegressor(n_jobs=-1, random_state=42)
    random_search = RandomizedSearchCV(xgb_model, param_distributions=param_dist, 
                                       n_iter=5, scoring='neg_mean_squared_error', 
                                       cv=3, verbose=1, random_state=42, n_jobs=-1)
    
    random_search.fit(X_train, y_train)
    best_xgb = random_search.best_estimator_
    logging.info(f"Melhores parâmetros XGBoost: {random_search.best_params_}")
    
    # Manter compatibilidade com código abaixo que usa model_xgb
    model_xgb = best_xgb

    # 10. Treinamento Random Forest (Baseline)
    logging.info("Treinando Random Forest Baseline...")
    rf_model = RandomForestRegressor(n_estimators=100, max_depth=10, n_jobs=-1, random_state=42)
    rf_model.fit(X_train, y_train)

    resultados = {}

    # 10. XAI - Importância padrão
    importances = model_xgb.feature_importances_
    feature_importances = sorted(
        zip(X_train.columns, importances),
        key=lambda x: x[1],
        reverse=True
    )

    print("\nImportância das features (XGBoost - gain):")
    for nome_feat, imp in feature_importances:
        print(f"{nome_feat:25s} -> {imp:.4f}")

    # 11. XAI - SHAP
    print("\nCalculando SHAP values (XAI)...")
    explainer = shap.TreeExplainer(model_xgb)

    amostra = X_train.sample(n=min(1000, len(X_train)), random_state=42)
    shap_values = explainer.shap_values(amostra)

    shap_importance = np.mean(np.abs(shap_values), axis=0)
    shap_feat_importances = sorted(
        zip(amostra.columns, shap_importance),
        key=lambda x: x[1],
        reverse=True
    )

    print("\nImportância das features (SHAP - |mean|):")
    for nome_feat, imp in shap_feat_importances[:30]:
        print(f"{nome_feat:25s} -> {imp:.4f}")

    # Se quiser gráfico:
    # shap.summary_plot(shap_values, amostra, show=True)

    # 13. Avaliação: converte yield previsto -> produção (XGBoost)
    y_pred_yield_test = best_xgb.predict(X_test)
    # Converter sc/ha -> toneladas: (sc/ha * area * 60) / 1000
    y_pred_prod_test  = (y_pred_yield_test * test_areas * 60) / 1000

    resultados['R2_2024_XGB'] = r2_score(y_test_prod_real, y_pred_prod_test)
    resultados['RMSE_2024_XGB'] = np.sqrt(mean_squared_error(y_test_prod_real, y_pred_prod_test))

    print(f"\nR² Safra 2024 (produção) XGBoost: {resultados['R2_2024_XGB']:.4f}")
    print(f"RMSE Safra 2024         XGBoost: {resultados['RMSE_2024_XGB']:.2f}")
    logging.info(f"R² Safra 2024 (produção) XGBoost: {resultados['R2_2024_XGB']:.4f}")
    logging.info(f"RMSE Safra 2024         XGBoost: {resultados['RMSE_2024_XGB']:.2f}")

    # 14. Avaliação Random Forest
    y_pred_yield_rf = rf_model.predict(X_test)
    y_pred_prod_rf = (y_pred_yield_rf * test_areas * 60) / 1000
    
    resultados['R2_2024_RF'] = r2_score(y_test_prod_real, y_pred_prod_rf)
    resultados['RMSE_2024_RF'] = np.sqrt(mean_squared_error(y_test_prod_real, y_pred_prod_rf))
    
    print(f"\nR² Safra 2024 (produção) Random Forest: {resultados['R2_2024_RF']:.4f}")
    print(f"RMSE Safra 2024         Random Forest: {resultados['RMSE_2024_RF']:.2f}")
    logging.info(f"R² Safra 2024 (produção) Random Forest: {resultados['R2_2024_RF']:.4f}")
    logging.info(f"RMSE Safra 2024         Random Forest: {resultados['RMSE_2024_RF']:.2f}")

    resultados['feature_importances_xgb'] = feature_importances
    resultados['feature_importances_shap'] = shap_feat_importances

    return resultados


# ================= EXECUÇÃO =================
resumo_final = {}

for nome, caminho in arquivos.items():
    res = rodar_xgb(nome, caminho)
    if res is not None:
        resumo_final[nome] = res

print("\n\n" + "="*40)
print("RESUMO FINAL: XGBoost PURO")
print("="*40)
for nome, metrics in resumo_final.items():
    if 'R2_2024_XGB' in metrics:
        print(f"{nome: <10} | XGB R²: {metrics['R2_2024_XGB']:.4f} | RF R²: {metrics.get('R2_2024_RF', 0):.4f}")
    else:
        print(f"{nome: <10} | R² 2024: {metrics.get('R2_2024', 0):.4f}")

import pandas as pd
import numpy as np
import xgboost as xgb
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, Masking, Dropout, Bidirectional, Flatten
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, r2_score
import os

# ==== NOVO: XAI (SHAP) ====
import shap
import matplotlib.pyplot as plt

# para alguns backends, ajuda a inicializar visualizações JS (opcional)
# shap.initjs()

# ================= CONFIGURAÇÃO =================
# base_dir = '/content/drive/MyDrive/dataset'
base_dir = './data'
arquivos = {
    "DIÁRIO": os.path.join(base_dir, 'soja_clima_ndvi_diario_final.csv'),
    "MENSAL": os.path.join(base_dir, 'soja_clima_ndvi_mensal_final.csv'),
    "ANUAL":  os.path.join(base_dir, 'soja_clima_ndvi_anual_final.csv')
}

EMBEDDING_SIZE = 32   # Tamanho do vetor latente

def rodar_hibrido(nome_dataset, caminho_csv):
    logging.info(f"{'#'*20} PROCESSANDO: {nome_dataset} {'#'*20}")

    # 1. Definição Dinâmica da Janela Temporal
    if nome_dataset == "DIÁRIO":
        MAX_TIMESTEPS = 180  # Ciclo da soja em dias
    elif nome_dataset == "MENSAL":
        MAX_TIMESTEPS = 12   # Máximo de meses numa safra
    else: # ANUAL
        MAX_TIMESTEPS = 1    # 1 linha por ano

    # 2. Carga e Limpeza
    try:
        df = pd.read_csv(caminho_csv)
        logging.info(f"CSV carregado: {caminho_csv}. Shape: {df.shape}")
    except FileNotFoundError:
        logging.error(f"Arquivo não encontrado: {caminho_csv}")
        return None

    # Ordenação
    cols_ordenacao = ['municipio', 'SAFRA']
    if 'data' in df.columns:
        df['data'] = pd.to_datetime(df['data'])
        cols_ordenacao.append('data')
    elif 'anomes' in df.columns:
        cols_ordenacao.append('anomes')

    df = df.sort_values(by=cols_ordenacao)

    # Limpar alvo e criar target PRODUTIVIDADE (Kg/ha)
    for col in ['PRODUCAO', 'AREA_TOTAL']:
        if df[col].dtype == 'object':
            df[col] = df[col].str.replace(',', '.')

    df['PRODUCAO'] = pd.to_numeric(df['PRODUCAO'], errors='coerce')
    df['AREA_TOTAL'] = pd.to_numeric(df['AREA_TOTAL'], errors='coerce')
    df = df.dropna(subset=['PRODUCAO', 'AREA_TOTAL'])
    df = df[df['AREA_TOTAL'] > 0] # Evitar divisão por zero
    logging.info(f"Shape após limpeza de target: {df.shape}")

    # --- ENGENHARIA DO ALVO ---
    # YIELD_REAL agora em sacas/ha (1 saca = 60kg = 0.06t)
    # Se PRODUCAO está em toneladas: (PRODUCAO * 1000 / 60) / AREA
    df['YIELD_REAL'] = (df['PRODUCAO'] * 1000 / 60) / df['AREA_TOTAL']
    logging.info(f"Target YIELD_REAL (sc/ha) calculado. Média: {df['YIELD_REAL'].mean():.2f}")

    # 3. Preparação das Features
    
    # Adicionar features agronômicas
    df = add_agronomic_features(df)
    
    # Para o modelo Híbrido (LSTM), features "acumuladas" podem ser redundantes se o LSTM aprender a memória.
    # Mas GDD e VPD instantâneos ajudam.
    # Vamos adicionar as acumuladas também para dar "dicas" explícitas ao modelo.
    if 'municipio' in df.columns and 'Chuva (mm)' in df.columns:
        # Ordenação já foi feita acima (linhas 726)
        for window in [30, 60, 90]:
             df[f'Chuva_Acum_{window}d'] = df.groupby('municipio')['Chuva (mm)'].transform(lambda x: x.rolling(window, min_periods=1).sum())
             df[f'GDD_Acum_{window}d'] = df.groupby('municipio')['GDD'].transform(lambda x: x.rolling(window, min_periods=1).sum())

    # --- INTERACTION FEATURES ---
    # Capturar sinergia: Calor só é bom com Água.
    if 'Chuva_Acum_90d' in df.columns and 'GDD_Acum_90d' in df.columns:
        df['Interacao_Chuva_GDD'] = df['Chuva_Acum_90d'] * df['GDD_Acum_90d']

    features_clima = ['Tmax (°C)', 'Tmin (°C)', 'Tmed (°C)', 'UR (%)',
                      'U2 (m/s)', 'RS (MJ/m²d)', 'Chuva (mm)', 'NDVI',
                      'GDD', 'VPD',
                      'Chuva_Acum_30d', 'Chuva_Acum_60d', 'Chuva_Acum_90d',
                      'GDD_Acum_30d', 'GDD_Acum_60d', 'GDD_Acum_90d',
                      'Interacao_Chuva_GDD',
                      'Heat_Stress_30d', 'Seca_Reprodutiva', 'Dias_Secos_30d',
                      'NDVI_x_Chuva', 'NDVI_x_GDD', 'NDVI_x_RS',
                      'Tmed_Quadrado', 'NDVI_Quadrado', 'Desvio_Temp_Otima',
                      'Chuva_Ano_Anterior', 'NDVI_Ano_Anterior',
                      'Reserva_Hidrica_Ano_Anterior', 'Chuva_Anual_Anterior']

    cols_reais = [c for c in features_clima if c in df.columns]
    logging.info(f"Features detectadas ({len(cols_reais)}): {cols_reais}")

    if not cols_reais:
        logging.error("Nenhuma feature climática encontrada.")
        return None

    # Normalização
    scaler = MinMaxScaler()
    df[cols_reais] = scaler.fit_transform(df[cols_reais].fillna(0))

    # 4. Construção do Tensor 3D
    print(f"Criando tensores (Janela Máxima: {MAX_TIMESTEPS} passos)...")
    sequencias = []
    targets_yield = []
    targets_prod_absoluta = []
    areas_reais = []
    meta_data = []

    grupos = df.groupby(['municipio', 'SAFRA'])

    for (muni, safra), grupo in grupos:
        seq = grupo[cols_reais].values

        if len(seq) > MAX_TIMESTEPS:
            seq = seq[:MAX_TIMESTEPS]
        else:
            pad_width = MAX_TIMESTEPS - len(seq)
            seq = np.pad(seq, ((0, pad_width), (0, 0)), mode='constant')

        sequencias.append(seq)

        targets_yield.append(grupo['YIELD_REAL'].iloc[0])
        targets_prod_absoluta.append(grupo['PRODUCAO'].iloc[0])
        area = grupo['AREA_TOTAL'].iloc[0]
        areas_reais.append(area)
        solo = grupo['Solo'].iloc[0] if 'Solo' in grupo.columns else 'Desconhecido'

        safra_str = str(safra)
        is_2024 = 1 if (safra_str == '23/24' or safra_str == '2024' or safra == 2024) else 0

        meta_data.append([muni, safra, solo, area, is_2024])

    X_seq = np.array(sequencias)
    y_yield = np.array(targets_yield)
    y_prod_abs = np.array(targets_prod_absoluta)
    areas_vec = np.array(areas_reais)

    # --- RESIDUAL CONNECTION: Extrair features do último passo da sequência ---
    # Queremos passar GDD_Acum, Chuva_Acum, etc. para o XGBoost
    # X_seq shape: (N, Timesteps, Features)
    # Pegamos o último timestep válido (ou o último da janela)
    # Como fizemos padding 'constant' (zeros) à direita se fosse menor, o último dado real pode não ser o último índice.
    # Mas no loop de construção (linhas 907-911), se len > MAX, cortamos. Se len < MAX, pad no final.
    # O ideal é pegar a última linha ANTES do pad.
    # Simplificação: Pegar a média ou soma das features agronômicas?
    # Melhor: Pegar as features acumuladas do último dia da safra (que deve ser o último elemento antes do pad).
    
    # Para simplificar e ser robusto: Vamos recalcular os acumulados "globais" por safra/municipio e colocar no df_meta.
    # Ou mais fácil: extrair do X_seq a última linha não-zero.
    
    residual_features = []
    for seq in sequencias:
        # Encontrar último índice não zero (baseado em alguma feature que nunca é zero, ex: Tmax ou apenas pegar o último se for full)
        # Assumindo que o padding é zero e Tmax > 0.
        # Se fizemos pad no final, os zeros estão no fim.
        # Vamos pegar o maximo ao longo do tempo para acumulados (GDD_Acum cresce).
        # Para médias (Tmed), pegar a média.
        
        # Abordagem mais segura: Calcular estatísticas da sequência (Mean, Max, Sum) para passar ao XGB
        # Isso transforma o LSTM num "Feature Extractor Temporal" e o XGB num "Feature Aggregator".
        
        # Vamos pegar a Média de todas as features e o Máximo (para acumulados)
        seq_mean = np.mean(seq, axis=0)
        seq_max  = np.max(seq, axis=0) # Bom para GDD_Acum, Chuva_Acum
        
        # Concatenar média e max
        residual_features.append(np.concatenate([seq_mean, seq_max]))
        
    X_residual = np.array(residual_features)
    # Nomes das features residuais
    feat_res_names = [f"{c}_mean" for c in cols_reais] + [f"{c}_max" for c in cols_reais]
    
    df_residual = pd.DataFrame(X_residual, columns=feat_res_names)

    df_meta = pd.DataFrame(meta_data, columns=['municipio', 'SAFRA', 'Solo', 'AREA_TOTAL', 'is_2024'])
    print(f"Shape do Tensor: {X_seq.shape}")

    # 5. Treinamento Rede Neural (Arquitetura Ajustada)
    mask_treino = df_meta['is_2024'] == 0
    X_train_nn = X_seq[mask_treino]
    y_train_nn = y_yield[mask_treino]

    input_layer = Input(shape=(MAX_TIMESTEPS, len(cols_reais)))
    masked = Masking(mask_value=0.0)(input_layer)

    # --- LÓGICA CORRIGIDA ---
    if nome_dataset == "MENSAL":
        # MANTÉM CAMPEÃO: Bidirecional
        x = Bidirectional(LSTM(64, return_sequences=False))(masked)
        x = Dropout(0.1)(x)

    elif nome_dataset == "DIÁRIO":
        # Unidirecional sem Pooling
        x = LSTM(64, return_sequences=False)(masked)
        x = Dropout(0.2)(x)

    else: # ANUAL
        # Dense Network (Tabular)
        x = Dense(32, activation='relu')(masked)
        x = Flatten()(x)
        x = Dropout(0.1)(x)

    embedding_layer = Dense(EMBEDDING_SIZE, activation='relu', name='layer_embedding')(x)
    output_layer = Dense(1, activation='linear')(embedding_layer)

    model_nn = Model(inputs=input_layer, outputs=output_layer)
    model_nn.compile(optimizer='adam', loss='mse')

    # Early Stopping
    es = EarlyStopping(monitor='loss', patience=15, restore_best_weights=True)

    model_nn.fit(X_train_nn, y_train_nn, epochs=50, batch_size=32, verbose=0, callbacks=[es])

    # 6. Extração e XGBoost
    extractor = Model(inputs=model_nn.input, outputs=model_nn.get_layer('layer_embedding').output)
    embeddings = extractor.predict(X_seq, verbose=0)

    df_emb = pd.DataFrame(embeddings, columns=[f'emb_{i}' for i in range(EMBEDDING_SIZE)])
    
    # CONCATENAÇÃO HÍBRIDA: Meta + Embeddings + Residual Features (Explicit)
    df_final = pd.concat([df_meta, df_emb, df_residual], axis=1)

    df_final['TARGET_YIELD'] = y_yield
    df_final['REAL_PRODUCAO'] = y_prod_abs

    for col in ['municipio', 'SAFRA', 'Solo']:
        df_final[col] = df_final[col].astype('category').cat.codes

    df_test = df_final[df_final['is_2024'] == 1].copy()
    df_train = df_final[df_final['is_2024'] == 0].copy()

    X_train_xgb = df_train.drop(columns=['TARGET_YIELD', 'REAL_PRODUCAO', 'is_2024', 'AREA_TOTAL'], errors='ignore')
    y_train_xgb = df_train['TARGET_YIELD']

    X_test_xgb = df_test.drop(columns=['TARGET_YIELD', 'REAL_PRODUCAO', 'is_2024', 'AREA_TOTAL'], errors='ignore')
    y_test_real_prod = df_test['REAL_PRODUCAO']
    test_areas = df_test['AREA_TOTAL']

    # XGBoost com Tuning AGRESSIVO
    logging.info("Iniciando Tuning AGRESSIVO do XGBoost (Híbrido)...")
    param_dist = {
        'n_estimators': [200, 500, 800, 1000],
        'learning_rate': [0.005, 0.01, 0.03, 0.05, 0.1],
        'max_depth': [3, 5, 7, 10, 12],
        'min_child_weight': [1, 3, 5],
        'subsample': [0.6, 0.7, 0.8, 0.9],
        'colsample_bytree': [0.5, 0.7, 0.8, 0.9],
        'reg_alpha': [0, 0.1, 0.5],  # L1 regularization
        'reg_lambda': [1, 2, 5]     # L2 regularization
    }
    
    xgb_model = xgb.XGBRegressor(n_jobs=-1, random_state=42)
    random_search = RandomizedSearchCV(xgb_model, param_distributions=param_dist, 
                                       n_iter=25, scoring='neg_mean_squared_error', 
                                       cv=5, verbose=1, random_state=42, n_jobs=-1)
    
    random_search.fit(X_train_xgb, y_train_xgb)
    best_xgb = random_search.best_estimator_
    logging.info(f"Melhores parâmetros XGBoost (Híbrido): {random_search.best_params_}")

    
    # --- ENSEMBLE STACKING (XGBoost + Random Forest) ---
    # Treinar também um Random Forest nos mesmos dados (Embeddings + Residuals)
    logging.info("Treinando Random Forest para Ensemble...")
    rf_model = RandomForestRegressor(n_estimators=300, max_depth=10, n_jobs=-1, random_state=42)
    rf_model.fit(X_train_xgb, y_train_xgb)
    
    model_xgb = best_xgb

    # ========= IMPORTÂNCIA DAS FEATURES (XGBoost + SHAP) =========
    resultados = {}

    # Importância padrão do XGBoost
    importances = model_xgb.feature_importances_
    feature_importances = sorted(
        zip(X_train_xgb.columns, importances),
        key=lambda x: x[1],
        reverse=True
    )

    print("\nImportância das features (XGBoost - gain):")
    for nome_feat, imp in feature_importances:
        print(f"{nome_feat:20s} -> {imp:.4f}")

    # -------- XAI com SHAP --------
    print("\nCalculando SHAP values (XAI)...")
    explainer = shap.TreeExplainer(model_xgb)

    # amostra para não ficar pesado
    amostra = X_train_xgb.sample(n=min(1000, len(X_train_xgb)), random_state=42)
    shap_values = explainer.shap_values(amostra)

    shap_importance = np.mean(np.abs(shap_values), axis=0)
    shap_feat_importances = sorted(
        zip(amostra.columns, shap_importance),
        key=lambda x: x[1],
        reverse=True
    )

    print("\nImportância das features (SHAP - |mean|):")
    for nome_feat, imp in shap_feat_importances[:30]:  # top 30
        print(f"{nome_feat:20s} -> {imp:.4f}")

    # Se quiser visualizar no notebook:
    # shap.summary_plot(shap_values, amostra, show=True)

    # 7. Avaliação
    if len(X_test_xgb) > 0:
        # Predições Individuais
        pred_test_yield_xgb = model_xgb.predict(X_test_xgb)
        pred_test_yield_rf  = rf_model.predict(X_test_xgb)
        
        # ENSEMBLE (Média)
        pred_test_yield_ensemble = (pred_test_yield_xgb + pred_test_yield_rf) / 2.0
        
        # Converter sc/ha -> toneladas: (sc/ha * area * 60) / 1000
        pred_test_prod_tons = (pred_test_yield_ensemble * test_areas * 60) / 1000

        resultados['R2_2024'] = r2_score(y_test_real_prod, pred_test_prod_tons)
        resultados['RMSE_2024'] = np.sqrt(mean_squared_error(y_test_real_prod, pred_test_prod_tons))

        print(f"R² Safra 2024 (Ensemble Hybrid): {resultados['R2_2024']:.4f}")
        logging.info(f"R² Safra 2024 (Ensemble): {resultados['R2_2024']:.4f}, RMSE: {resultados['RMSE_2024']:.2f}")
    else:
        resultados['R2_2024'] = 0.0
        resultados['RMSE_2024'] = 0.0
        logging.warning("Sem dados de 2024.")

    # guardar importâncias no dict
    resultados['feature_importances_xgb'] = feature_importances
    resultados['feature_importances_shap'] = shap_feat_importances

    return resultados

# ================= EXECUÇÃO =================
resumo_final = {}

for nome, caminho in arquivos.items():
    res = rodar_hibrido(nome, caminho)
    if res:
        resumo_final[nome] = res

print("\n\n" + "="*40)
print("RESUMO FINAL: ARQUITETURA DEFINITIVA")
print("="*40)
for nome, metrics in resumo_final.items():
    print(f"{nome: <10} | R² 2024: {metrics['R2_2024']:.4f} | RMSE: {metrics['RMSE_2024']:.2f}")

















# ============================================================
